# Data-Bias-
This project explores data biases in machine learning practices through the use of perspective API's in checking toxicity of comments. 

# Goal of the Project
The goal of this project was to explore the concept of data biases in machine learning practices through the use of the Perspective API for toxicity of comments. The API dataset included the comment text, a unique id, and toxicity remarked as yes or no. 

# Exploring Sample Dataset to form hypotheses
After a quick visual inspection of some of the top comments of the dataset, it appeared that there could be potential bias towards LGTBQ members and members of varying religions. I used a Jupyter notebook using python in order to parse a sample dataset csv file containing the toxic comments. The correspondence of the column toxicity was either answered as ‘yes’ or ‘no’ which I then converted into integers where 1 would correspond to ‘yes’ and 0 corresponds to ‘no’. The larger comments that took up many lines also seemed to have some hidden toxic words buried deep but may have been reported as non-toxic dude to their lengh. The hypothesis I wanted to test was to see if the Perspective will fail if we replace the most common swear words with less common obscenities. 

# Form Hypotheses & Design & perform tests
This assignment tests the API’s model for bias by looking at performance of the dataset through original submitted comment texts and their corresponding toxicity.  I first used the Google Comment Analyzer API client from googleapiclient.discovery to make requests to the API concerning comment text and toxicity reports. I then established a threshold of .05 and evaluated its precision to be .55 and F1-score reported as .71. The F-1 Score of .711 indicated that the model does a well enough job in achieving precision and recall, however there of course will exist misclassifaction of some of the comments being classified as toxic when they aren't and vice versa. Overall the f-1 score suggests that the model performs somewhat well with plenty of room for improvement, especially in the area of precision. 

In order to test my hypothesis I used some sample comments from the 'sample labeled dataset' provided in Canvas. Next, I developed a small test of these sample example comments to test my hypothesis. I replaced the common swear words with corresponding less common obscenities. From there, I wanted to find there corresponding test scores. 

# Analysis
After looking at the concluding marks made by the data, I can infer that the Perspective was not sufficient in tailing out the lesser common obscenities. There are many biases that might be existent in the model. Some of these include context bias, tone bias, or a cultural bias. Of course, the Google API is trained on enormous amounts of datasets, however it may not be fully representative of all the cultures and languages it should be representing. Cultural and language barriers may be pertruding to this. Additionally, when analyzing comments tone bias is extremely important to consider. The model may be perceiving some more informal language to be negative straight off the bet, even if it isn't. The last bias I mention is the context bias. The API may be less accurate whe analyzing some ambigious comments and may not be able to fully comprehnd the scope of what is being said. We spend years in school learning about context clues to help derive meanings of unknown passages and this skill may be lacking in the model. 

What suprised me the most about my findings was the trouble the model had in detecting the less obscene swear words. But, this does make sense as these words are less prevalent in the training data used to develop the model, allowing them to be underrepresented. In order to address this issue the training data should be more broad to include a diverse range of langugae, even if it's not what we are seeing being printed in the actual comments of a post. Furthermore, this issue can filter in the more culture biases which can be seen as bigger issues. If theres more data and knowledge about culture A over culture B in a trained ML model, culture A is going to come out on top of each query more likely than the other.

# Further Questions
Before training a model based on particular datasets, how can we ensure these datasets are representative of the diverse array of variables it takes into account? Is there any way to do such yet? 
